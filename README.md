# deeplearning_MNIST

한국방송통신대학교 머신러닝 과제
```
python3 main.py
```
# 1. 예제 코드 결과








손실함수 : Cross Entropy Loss
컨볼루션 계층 1개
FC 계층 1개
최적화 알고리즘 : SGD
수행시간 : 2m 28s
초기 모델을 실행했을때 , 결과는 상당히 만족스러웠다. Cross Entropy Loss는 분류문제에서 일반적으로 사용되는 함수로 , 모델의 성능을 안정적으로 평가할 수 있다.





# 2. 계층 수 증가

코드를 짜던중 파라미터의 의미들을 이해를 못해 한참 헤맸었다. FC계층과 Convolution 계층을 각각 2개로 늘려 모델의 깊이를 증가시켰다 정확도는 93~97% 사이이며 매번 테스트결과가 다르게 나왔다. 딥러닝 특유의 불안정성으로 초기 가중치값에 영향을 받는것으로 판단된다. 그래도 다소 성능향상을 볼 수 있었다. 가끔은 과적합 문제로 원래 예제코드보다 성능이 떨어지는 경우도 발생했다.

평균수행시간 : 2m 32s



# 3 . 최적화 알고리즘 변경
그동안은 SGD를 사용해 왔으니 새로운 최적화 알고리즘으로 바꿔보겠다.

## 3.1 Momentum







이 기법은 관성을 집어 넣듯 이전 단계 그라디언트를 일정 비율로 섞어 더 빠르게 수렴하도록 돕는다. 지역 극소점에서 빠져 나오는데 기여를 하기에 손실함수 최적점과 테스트 결과가 눈에 띄게 향상됐다. 수행속도에서도 소폭의 향상이 보였으며, 관성으로 최적점 근처에서 한번씩 튀긴 했지만 전반적으로 결과의 안정성 또한 향상됐다.

평균수행시간 : 2m 27s



## 3.2 RMSProp







학습률을 적응적으로 조정한다. 학습이 빠르고 학습률 설정에 덜 민감하다.
SGD와 대비해 훨씬 나은 결과를 보여줬으며 손실함수 계산과정에서 결과들이 좀 튀는 경향이 있었다.
Momentum과 비교했을때 큰 차이는 보이지 않았으나 메모리 사용량이 늘었다.
평균수행시간 : 평균 2m 27s
## 3.3 Adam
Momentum과 RMSProp을 장점을 결합한 모델이다 수행속도가 소폭 빨라졌고 
수렴이 빠르고 최적점에서 관성으로 지나쳐 튀는 모습이 보이나 다시 극소점을 잘 찾아간다.
SGD에 비하면 훨씬 좋은 테스트 결과와 수행속도를 보여줬다.ㅇ
평균수행시간 : 2m 25s

# 4 . 모델 변형 








Relu대신 leaky_relu를 사용해 보았다. ReLU는 입력이 0 이하일 경우 기울기가 0이 되며, 이로 인해 해당 뉴런이 업데이트되지 않는 "죽은 뉴런" 현상이 발생할 수 있다. Leaky ReLU는 음수 입력에 대해 일정한 기울기를 이용해 정보손실을 줄여준다. 테스트결과 근소한 성능향상과 좀 더 안정적이고 덜 튀는 그래디언트 흐름이 눈에 띄었다.

평균수행시간 : 2m 25s 
# 5 . 최종
전체적인 성능향상에 가장 큰 영향을 준것은 최적화 모델 변형과, 계층의 깊이였다. 계층의 깊이를 변경함에 따라 표현할 수 있는 복잡도가 늘어 성능향상에 가속도를 붙일 수 있었고, 그것이 최적화 모델의 변형과 어우려져 훨씬 안정적인 결과를 얻을 수 있었다. 
